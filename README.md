
# Projeto 01 - ComputaÃ§Ã£o Paralela
## ImplementaÃ§Ã£o Paralela de K-Means com OpenMP e MPI

[![C++17](https://img.shields.io/badge/C%2B%2B-17-blue.svg)](https://isocpp.org/)
[![OpenMP](https://img.shields.io/badge/OpenMP-4.5+-green.svg)](https://www.openmp.org/)
[![MPI](https://img.shields.io/badge/MPI-3.0+-orange.svg)](https://www.mpi-forum.org/)

---

## ğŸ“‹ Sobre o Projeto

Este projeto implementa o **algoritmo K-Means clustering** utilizando tÃ©cnicas de paralelizaÃ§Ã£o:
- **OpenMP**: paralelismo de memÃ³ria compartilhada
- **MPI + OpenMP**: paralelismo hÃ­brido (memÃ³ria distribuÃ­da + compartilhada)

### O que Ã© K-Means?
K-Means Ã© um algoritmo de aprendizado de mÃ¡quina nÃ£o supervisionado que agrupa dados em K clusters, minimizando a distÃ¢ncia euclidiana entre os pontos e os centrÃ³ides de seus respectivos clusters. Ã‰ amplamente utilizado em anÃ¡lise de dados, reconhecimento de padrÃµes e compressÃ£o de imagens.

### Dataset Utilizado
**MNIST** (Modified National Institute of Standards and Technology)
- 60.000 imagens de dÃ­gitos manuscritos (0-9)
- 784 features por imagem (28Ã—28 pixels)
- Formato CSV para processamento eficiente
- Download: [Kaggle - MNIST in CSV](https://www.kaggle.com/datasets/oddrationale/mnist-in-csv)

### Desenvolvimento
CÃ³digo desenvolvido especificamente para este projeto educacional, implementando K-Means com extensÃµes para paralelizaÃ§Ã£o OpenMP e MPI.


---

## ğŸ“ Estrutura do Projeto

```
Projeto01_Computacao_Paralela/
â”œâ”€â”€ main.cc                    # ImplementaÃ§Ã£o OpenMP
â”œâ”€â”€ main_hybrid.cc             # ImplementaÃ§Ã£o hÃ­brida MPI+OpenMP
â”œâ”€â”€ CMakeLists.txt             # ConfiguraÃ§Ã£o de build
â”œâ”€â”€ README.md                  # Este arquivo
â”œâ”€â”€ TESTES.txt                 # Resultados dos benchmarks
â”œâ”€â”€ run_benchmarks.sh          # Script automatizado de testes
â”œâ”€â”€ run_quick_test.sh          # Script de teste rÃ¡pido
â”œâ”€â”€ BENCHMARK_SCRIPTS.txt      # DocumentaÃ§Ã£o dos scripts
â”œâ”€â”€ QUICK_START.txt            # Guia de inÃ­cio rÃ¡pido
â”œâ”€â”€ example_output.csv         # Exemplo de saÃ­da CSV
â”œâ”€â”€ dataset/
â”‚   â”œâ”€â”€ mnist_train.csv        # Dataset principal (60k amostras) - nÃ£o versionado
â”‚   â””â”€â”€ mnist_test.csv         # Dataset de teste (10k amostras)
â””â”€â”€ build/                     # DiretÃ³rio de compilaÃ§Ã£o (gerado)
    â”œâ”€â”€ main                   # ExecutÃ¡vel OpenMP
    â””â”€â”€ main_hybrid            # ExecutÃ¡vel MPI+OpenMP
```

---

## ğŸ”§ Requisitos

### Software NecessÃ¡rio
- **CMake** 3.15 ou superior
- **Compilador C++** com suporte a C++17 (g++ 7+, clang++ 5+)
- **OpenMP** 4.5 ou superior
- **MPI** (OpenMPI 3.0+ ou MPICH 3.2+)

### Dataset
- MNIST em formato CSV
- Download disponÃ­vel em: https://www.kaggle.com/datasets/oddrationale/mnist-in-csv
- Extrair os arquivos na pasta `dataset/`

### VerificaÃ§Ã£o dos Requisitos
```bash
# Verificar CMake
cmake --version

# Verificar compilador C++
g++ --version

# Verificar OpenMP
echo | cpp -fopenmp -dM | grep -i open

# Verificar MPI
mpirun --version
```

---


## ğŸ”¨ CompilaÃ§Ã£o

### MÃ©todo 1: CMake (Recomendado)

```bash
# Configurar o projeto
cmake -S . -B build -DCMAKE_BUILD_TYPE=Release

# Compilar
cmake --build build

# Ou em uma linha
cmake -S . -B build && cmake --build build
```

**ExecutÃ¡veis gerados:**
- `build/main` - VersÃ£o OpenMP
- `build/main_hybrid` - VersÃ£o MPI+OpenMP

### MÃ©todo 2: CompilaÃ§Ã£o Manual

```bash
# VersÃ£o OpenMP
g++ -fopenmp -O3 -std=c++17 -o kmeans_openmp main.cc

# VersÃ£o HÃ­brida MPI+OpenMP
mpicxx -fopenmp -O3 -std=c++17 -o kmeans_hybrid main_hybrid.cc
```

---


## ğŸš€ ExecuÃ§Ã£o

### Sintaxe Geral

```bash
./executavel <dataset.csv> <K> <max_iter> [seed]
```

**ParÃ¢metros:**
- `dataset.csv` - Arquivo CSV com os dados (cada linha Ã© um ponto)
- `K` - NÃºmero de clusters desejado
- `max_iter` - NÃºmero mÃ¡ximo de iteraÃ§Ãµes
- `seed` - Semente aleatÃ³ria (opcional, para reprodutibilidade)

### Exemplos de Uso

#### 1ï¸âƒ£ VersÃ£o OpenMP

```bash
# Com 4 threads
OMP_NUM_THREADS=4 ./build/main dataset/mnist_train.csv 10 100

# Teste com diferentes nÃºmeros de threads
OMP_NUM_THREADS=1 ./build/main dataset/mnist_train.csv 15 100
OMP_NUM_THREADS=2 ./build/main dataset/mnist_train.csv 15 100
OMP_NUM_THREADS=4 ./build/main dataset/mnist_train.csv 15 100
OMP_NUM_THREADS=8 ./build/main dataset/mnist_train.csv 15 100
```

#### 2ï¸âƒ£ VersÃ£o HÃ­brida MPI+OpenMP

```bash
# 1 processo Ã— 4 threads = 4 cores
OMP_NUM_THREADS=4 mpirun -np 1 ./build/main_hybrid dataset/mnist_train.csv 15 100

# 2 processos Ã— 2 threads = 4 cores
OMP_NUM_THREADS=2 mpirun -np 2 ./build/main_hybrid dataset/mnist_train.csv 15 100

# 4 processos Ã— 1 thread = 4 cores (MPI puro)
mpirun -np 4 ./build/main_hybrid dataset/mnist_train.csv 15 100
```

#### 3ï¸âƒ£ Usando Scripts Automatizados

```bash
# Executar todos os benchmarks (K=15, 100 iteraÃ§Ãµes)
./run_benchmarks.sh

# Teste rÃ¡pido (K=5, 50 iteraÃ§Ãµes)
./run_benchmarks.sh -k 5 -m 50

# Benchmark customizado com saÃ­da especÃ­fica
./run_benchmarks.sh -k 10 -m 100 -o meus_resultados.csv

# Teste rÃ¡pido para desenvolvimento
./run_quick_test.sh
```

**OpÃ§Ãµes do `run_benchmarks.sh`:**
```
-k, --clusters NUM     NÃºmero de clusters (padrÃ£o: 15)
-m, --max-iter NUM     MÃ¡ximo de iteraÃ§Ãµes (padrÃ£o: 100)
-s, --seed NUM         Semente aleatÃ³ria (padrÃ£o: 42)
-o, --output FILE      Arquivo CSV de saÃ­da (padrÃ£o: benchmark_results.csv)
-d, --dataset PATH     Caminho do dataset (padrÃ£o: dataset/mnist_train.csv)
-h, --help             Mostrar ajuda
```

---


## ğŸ“Š Resultados de Desempenho

### Ambiente de Teste
- **Servidor**: parcode
- **Dataset**: MNIST train (60.000 amostras, 785 dimensÃµes)
- **ConfiguraÃ§Ã£o**: K=15 clusters, max_iter=100
- **MediÃ§Ã£o**: Tempo total de execuÃ§Ã£o atÃ© convergÃªncia

### Resultados - OpenMP

| Threads | Tempo (s) | Speedup | EficiÃªncia |
|---------|-----------|---------|------------|
| 1       | 51.46     | 1.00x   | 100%       |
| 2       | 28.02     | 1.84x   | 92%        |
| 4       | 17.37     | 2.96x   | 74%        |
| 8       | 12.28     | 4.19x   | 52%        |

### Resultados - MPI+OpenMP HÃ­brido (4 cores totais)

| ConfiguraÃ§Ã£o      | Processos | Threads | Tempo (s) | Speedup |
|-------------------|-----------|---------|-----------|---------|
| 1 proc Ã— 4 thr    | 1         | 4       | 48.65     | 1.06x   |
| 2 proc Ã— 2 thr    | 2         | 2       | 28.01     | 1.84x   |
| 4 proc Ã— 1 thr    | 4         | 1       | 28.10     | 1.83x   |

### AnÃ¡lise dos Resultados

âœ… **OpenMP mostrou excelente escalabilidade:**
- Speedup quase linear atÃ© 4 threads (2.96x)
- Speedup de 4.19x com 8 threads demonstra boa utilizaÃ§Ã£o de recursos
- EficiÃªncia de 74% com 4 threads e 52% com 8 threads

âš ï¸ **MPI+OpenMP hÃ­brido:**
- Overhead de comunicaÃ§Ã£o MPI impacta o desempenho
- Melhor resultado com 2 processos Ã— 2 threads (1.84x)
- Para este problema, OpenMP puro Ã© mais eficiente

ğŸ“ˆ **ConclusÃ£o**: Para datasets que cabem em memÃ³ria compartilhada, OpenMP oferece melhor desempenho. MPI+OpenMP Ã© vantajoso para datasets maiores que nÃ£o cabem em um Ãºnico nÃ³.

> **Nota**: Resultados completos com K=5, K=10 e K=15 disponÃ­veis em `TESTES.txt`

---


## ğŸ§ª Scripts de Teste Automatizados

### ğŸ¯ `run_benchmarks.sh` - Benchmark Completo

Script automatizado que executa todos os testes requeridos e gera relatÃ³rio CSV.

**Funcionalidades:**
- âœ… Compila automaticamente ambas as versÃµes
- âœ… Executa testes OpenMP (1, 2, 4, 8 threads)
- âœ… Executa testes MPI+OpenMP (1Ã—4, 2Ã—2, 4Ã—1)
- âœ… Salva resultados em CSV estruturado
- âœ… Calcula e exibe anÃ¡lise de speedup
- âœ… ValidaÃ§Ã£o de erros e feedback colorido

**Uso bÃ¡sico:**
```bash
./run_benchmarks.sh                    # Usa configuraÃ§Ã£o padrÃ£o (K=15, iter=100)
./run_benchmarks.sh -k 10 -m 100       # K=10, 100 iteraÃ§Ãµes
./run_benchmarks.sh -k 5 -m 50 -o test.csv  # Teste rÃ¡pido com saÃ­da customizada
./run_benchmarks.sh --help             # Ver todas as opÃ§Ãµes
```

**Tempo estimado:** ~15-20 minutos para K=15 com dataset completo

### âš¡ `run_quick_test.sh` - Teste RÃ¡pido

Script para validaÃ§Ã£o rÃ¡pida durante desenvolvimento.

**ConfiguraÃ§Ã£o:**
- K=5 clusters
- 50 iteraÃ§Ãµes mÃ¡ximas
- 4 threads OpenMP
- Apenas versÃ£o OpenMP

**Tempo estimado:** ~30 segundos

```bash
./run_quick_test.sh
```

> ğŸ“– **DocumentaÃ§Ã£o completa:** Veja `BENCHMARK_SCRIPTS.txt` para detalhes sobre os scripts

---


## ğŸ” Detalhes da ImplementaÃ§Ã£o

### Algoritmo K-Means

1. **InicializaÃ§Ã£o**: Seleciona K centrÃ³ides aleatÃ³rios
2. **Assignment**: Atribui cada ponto ao centrÃ³ide mais prÃ³ximo
3. **Update**: Recalcula centrÃ³ides como mÃ©dia dos pontos atribuÃ­dos
4. **ConvergÃªncia**: Repete passos 2-3 atÃ© centrÃ³ides estabilizarem

### ParalelizaÃ§Ã£o OpenMP (`main.cc`)

**EstratÃ©gia:**
- Loop principal paralelizado com `#pragma omp parallel for`
- Cada thread processa subconjunto de pontos
- Acumuladores locais por thread evitam contenÃ§Ã£o
- ReduÃ§Ã£o manual com seÃ§Ã£o crÃ­tica

**Diretivas utilizadas:**
```cpp
#pragma omp parallel              // RegiÃ£o paralela
#pragma omp for schedule(static)  // DivisÃ£o estÃ¡tica do trabalho
#pragma omp critical              // ProteÃ§Ã£o da reduÃ§Ã£o
```

**OtimizaÃ§Ãµes:**
- Scheduling estÃ¡tico para balanceamento de carga
- Buffers locais minimizam sincronizaÃ§Ã£o
- CÃ¡lculo de distÃ¢ncia euclidiana otimizado

### ParalelizaÃ§Ã£o MPI+OpenMP (`main_hybrid.cc`)

**Arquitetura hÃ­brida de dois nÃ­veis:**

**NÃ­vel 1 - MPI (entre nÃ³s):**
- `MPI_Scatterv`: Distribui dados entre processos
- `MPI_Bcast`: Sincroniza centrÃ³ides
- `MPI_Allreduce`: ReduÃ§Ã£o global de somas/contagens
- `MPI_Gatherv`: Coleta resultados finais

**NÃ­vel 2 - OpenMP (dentro do nÃ³):**
- Threads paralelizam cÃ¡lculo de distÃ¢ncias
- Buffers por thread evitam race conditions
- ReduÃ§Ã£o local antes da reduÃ§Ã£o MPI

**Fluxo de execuÃ§Ã£o:**
```
1. Rank 0 carrega dataset â†’ MPI_Bcast metadados â†’ MPI_Scatterv distribui dados
2. Loop de iteraÃ§Ãµes:
   a. OpenMP paralelo: cada thread calcula distÃ¢ncias localmente
   b. ReduÃ§Ã£o OpenMP: combina resultados das threads
   c. MPI_Allreduce: combina resultados entre processos
   d. AtualizaÃ§Ã£o de centrÃ³ides (replicado em todos os processos)
   e. VerificaÃ§Ã£o de convergÃªncia global
3. MPI_Gatherv: Rank 0 coleta labels finais
```

---

## ğŸ’¡ ObservaÃ§Ãµes Importantes

### Performance
- â±ï¸ Tempo de execuÃ§Ã£o varia com K (mais clusters = mais tempo)
- ğŸ¯ ConvergÃªncia ocorre quando centrÃ³ides estabilizam (diferenÃ§a < 1e-6)
- ğŸ”„ Para reprodutibilidade, use a mesma seed em todas as execuÃ§Ãµes

### LimitaÃ§Ãµes
- ğŸ’¾ Dataset completo deve caber na memÃ³ria (versÃ£o OpenMP)
- ğŸŒ Overhead de comunicaÃ§Ã£o MPI pode superar benefÃ­cios em datasets pequenos
- ğŸ“Š Melhor escalabilidade observada com K â‰¥ 10

### Dataset
- âš ï¸ `mnist_train.csv` (105MB) nÃ£o estÃ¡ versionado no Git (excede limite GitHub)
- âœ… `mnist_test.csv` (18MB) incluÃ­do para testes rÃ¡pidos
- ğŸ“¥ Download do dataset completo: [Kaggle - MNIST CSV](https://www.kaggle.com/datasets/oddrationale/mnist-in-csv)

---

## ğŸ“š DocumentaÃ§Ã£o Adicional

- **`main.cc`**: ComentÃ¡rios detalhados sobre paralelizaÃ§Ã£o OpenMP
- **`main_hybrid.cc`**: DocumentaÃ§Ã£o completa da implementaÃ§Ã£o hÃ­brida
- **`TESTES.txt`**: Resultados completos dos benchmarks
- **`BENCHMARK_SCRIPTS.txt`**: Guia completo dos scripts de teste
- **`QUICK_START.txt`**: Guia rÃ¡pido para comeÃ§ar

---

